# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2020, AdvanceSoft Corporation
# This file is distributed under the same license as the Advance/NeuralMD
# package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2020.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: Advance/NeuralMD \n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2022-09-05 15:25+0900\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.8.0\n"

#: ../../theory.rst:5
msgid "理論"
msgstr "Theory"

#: ../../theory.rst:10
msgid "対称関数"
msgstr "Symmetry Function"

#: ../../theory.rst:12
msgid "原子が置かれている状況（具体的には、近傍の原子構造）をニューラルネットワークに入力できる形に変換するのが、対称関数です。原子のエネルギーは系の平行移動や回転、同種原子の入れ替えに対して不変（対称）であるため、エネルギーに関する特徴量を抽出する対称関数に対しても、同じ性質が要請されます。また、ニューラルネットワークの入力ノード数が固定なので、対称関数の値の数も近傍原子数に依らず一定である必要があります。例えば「近傍原子との距離」や「近傍原子のなす角度」は、平行移動や回転に対しては不変ですが、近傍原子数が増えればその数だけ値が増えるため、そのまま使うことはできません。"
msgstr ""
"A symmetry function converts the environment around an atom (specifically, adjacent atomic structures) into the format that can be used as inputs of a neural network. "
"Because the atomic energy is constant (symmetric) through translations, rotations of the system and replacements between homologous atoms, a symmetry function, which extract features related to energy needs to be the similar characteristics. "
"Besides, the number of values of a symmetry function must be constant regardless of the number of adjacent atoms because the number of input nodes of a neural network is fixed. "
"For example, \"the distance to adjacent atoms\" and \"the angle formed by adjacent atoms\" are constant through translations and rotations but unavailable as they are because the number of values increase accordingly with the number of adjacent atoms."

#: ../../theory.rst:14
msgid ""
"対称関数としては様々なものが提案されていますが、本製品ではBehlerが提案\\ [1]_\\ した対称関数（\\ :math:`G^2`\\ "
"：動径成分、\\ :math:`G^3`\\ または\\ :math:`G^4`\\ ：角度成分）："
msgstr ""
"Although there are various ones proposed as symmetry functions, in this product, the symmetry function proposed by Behler\\ [1]_ (\\ :math:`G^2`\\ : the radial component, :math:`G^3` or :math:`G^4`\\ : the angular component):"

#: ../../theory.rst:16
msgid ""
"f_c(R_{ij}) &=\n"
"\\begin{cases}\n"
"\\tanh^3\\left[ 1 - \\frac{R_{ij}}{R_c} \\right] \\text{or} \\; "
"\\frac{1}{2}\\left[ \\cos\\left(\\frac{\\pi R_{ij}}{R_c}\\right)+1 "
"\\right] &\\text{for} \\; R_{ij}\\leq R_c \\\\\n"
"0 &\\text{for} \\; R_{ij} > R_c,\n"
"\\end{cases} \\\\\n"
"G_i^2 &= \\sum_{j=1}^{N_\\text{atom}} e^{-\\eta (R_{ij}-R_s)^2} \\cdot "
"f_c(R_{ij}), \\\\\n"
"G_i^3 &= 2^{1-\\zeta} \\sum_{j \\neq i} \\sum_{k \\neq i,j} \\left[ "
"(1+\\lambda \\cdot \\cos\\theta_{ijk})^\\zeta \\cdot e^{-\\eta \\left[ "
"(R_{ij}-R_s)^2+(R_{ik}-R_s)^2+(R_{jk}-R_s)^2 \\right]} \\cdot f_c(R_{ij})"
" \\cdot f_c(R_{ik}) \\cdot f_c(R_{jk}) \\right], \\\\\n"
"G_i^4 &= 2^{1-\\zeta} \\sum_{j \\neq i} \\sum_{k \\neq i,j} \\left[ "
"(1+\\lambda \\cdot \\cos\\theta_{ijk})^\\zeta \\cdot e^{-\\eta \\left[ "
"(R_{ij}-R_s)^2+(R_{ik}-R_s)^2 \\right]} \\cdot f_c(R_{ij}) \\cdot "
"f_c(R_{ik}) \\right],"
msgstr ""

#: ../../theory.rst:27
msgid ""
"Chebyshev多項式を使った対称関数\\ [2]_\\ （\\ :math:`c_\\alpha^{(2)}`\\ ：動径成分、\\ "
":math:`c_\\alpha^{(3)}`\\ ：角度成分）："
msgstr ""
"the symmetry function using the Chebyshev polynomials\\ [2]_ (\\ :math:`c_\alpha^{(2)}`\\ : the radial component, :math:`c_\alpha^{(3)}`\\ : the angular component):"

#: ../../theory.rst:29
msgid ""
"T_0(x) &= 1, \\\\\n"
"T_1(x) &= x, \\\\\n"
"T_{n+1}(x) &= 2xT_n(x)-T_{n-1}(x), \\\\\n"
"c_\\alpha^{(2)} &= \\sum_{j \\neq i} T_\\alpha "
"\\left(\\frac{2R_{ij}}{R_c}-1 \\right)f_c(R_{ij}), \\\\\n"
"c_\\alpha^{(3)} &= \\sum_{j \\neq i} \\sum_{k \\neq i,j} T_\\alpha "
"\\left(\\frac{2\\theta_{ijk}}{\\pi}-1 \\right)f_c(R_{ij})f_c(R_{ik}),"
msgstr ""

#: ../../theory.rst:38
msgid ""
"および、SANNPの論文\\ [3]_\\ で使われている対称関数（\\ :math:`G^{(2)}`\\ ：2体成分、\\ "
":math:`G^{(3)}`\\ ：3体成分）："
msgstr ""
"and the symmetry function described in the paper of SANNP\\ [3]_ (\\ :math:`G^{(2)}`\\ : the two-body component, :math:`G^{(3)}`\\ : the three-body component):"

#: ../../theory.rst:40
msgid ""
"R_\\alpha^k &= R_\\text{inner} + (\\alpha - 1)h_k, \\\\\n"
"\\text{where} \\; \\alpha &= 1,2,...,M_k, \\\\\n"
"\\varphi_\\alpha^{(k)}(R_{ml}) &=\n"
"\\begin{cases}\n"
"\\frac{1}{2}\\cos\\left(\\frac{R_{ml}-R_\\alpha^k}{h_k}\\pi\\right) + "
"\\frac{1}{2}, & | R_{ml}-R_\\alpha^k | < h_k\\\\\n"
"0, & \\text{Otherwise},\n"
"\\end{cases} \\\\\n"
"G_{\\alpha,l}^{(2)} &= \\sum_m \\varphi_\\alpha^{(2)}(R_{ml}), \\\\\n"
"G_{\\alpha\\beta\\gamma,l}^{(3)} &= \\sum_{m,n} "
"\\varphi_\\alpha^{(3)}(R_{ml})\\varphi_\\beta^{(3)}(R_{nl})\\varphi_\\gamma^{(3)}(R_{mn}),"
" \\\\\n"
"h_k &= (R_\\text{outer}-R_\\text{inner})/M_k"
msgstr ""

#: ../../theory.rst:53
msgid "（本製品ではMany-Body対称関数と呼びます）が使用可能です。"
msgstr "(it is called the Many-Body symmetry function in this product) are available."

#: ../../theory.rst:55
msgid ""
"系に異なる複数の元素が含まれる場合には、元素の組み合わせごとに対称関数\\ :math:`\\phi`\\ "
"（のパラメータセット）を用意することでその違いを表現する、という方法があります："
msgstr ""
"In the case that a system contains several different elements, there is the method to express the difference of elements "
"by preparing a symmetry function :math:`\\phi` (its parameter set) for each combination of elements:"

#: ../../theory.rst:57
msgid ""
"G_i^2 &= \\sum_{j \\neq i} \\phi_{Z_i, Z_j}^{(2)} (\\mbox{\\boldmath "
"$r$}_i, \\mbox{\\boldmath $r$}_j) ,\\\\\n"
"G_i^3 &= \\sum_{j \\neq i} \\sum_{k \\neq i,j} \\phi_{Z_i, Z_j, "
"Z_k}^{(3)} (\\mbox{\\boldmath $r$}_i, \\mbox{\\boldmath $r$}_j, "
"\\mbox{\\boldmath $r$}_k) ."
msgstr ""

#: ../../theory.rst:62
msgid ""
"この方法では、系に含まれる元素の種類が増えると、その組み合わせの数だけ多数のパラメータが必要になってしまう、という問題があります。これを避けるため、対称関数は増やさずに共通のものを使い、かわりに元素ごとの「重み」を導入するという方法\\"
" [2]_\\ [4]_\\ が提案されています："
msgstr ""
"There is the problem in this method that required parameters increase by the number of combinations with increase of the number of types of atoms in the system. "
"To avoid it, the method that introduces the \"weight\" for each element while using the same symmetry function\\ [2]_\\ [4]_ instead of increasing the number of symmetry functions has been proposed."

#: ../../theory.rst:64
msgid ""
"G_i^2 &= \\sum_{j \\neq i} g(Z_j) \\phi^{(2)} (\\mbox{\\boldmath $r$}_i, "
"\\mbox{\\boldmath $r$}_j) ,\\\\\n"
"G_i^3 &= \\sum_{j \\neq i} \\sum_{k \\neq i,j} h(Z_j, Z_k) \\phi^{(3)} "
"(\\mbox{\\boldmath $r$}_i, \\mbox{\\boldmath $r$}_j, \\mbox{\\boldmath "
"$r$}_k) ."
msgstr ""

#: ../../theory.rst:69
msgid ""
"本製品では、Behler対称関数とChebyshev対称関数でこの重み付き対称関数が使用可能です。重みとしては、原子番号 "
":math:`g(Z_i)=Z_i` およびその幾何平均 :math:`h(Z_i, Z_j)=\\sqrt{Z_i Z_j}` を使用します。"
msgstr ""
"In this product, this weighted symmetry function can be used with the Behler symmetry function and the Chebyshev symmetry function. "
"Atomic number :math:`g(Z_i)=Z_i` and geometric mean :math:`h(Z_i, Z_j)=\\sqrt{Z_i Z_j}` are used as the weight."

#: ../../theory.rst:74
msgid "ニューラルネットワーク"
msgstr "Neural Network"

#: ../../theory.rst:76
msgid "ニューラルネットワークは、入力層、1つ以上の中間層（隠れ層）、出力層からなります。各層には1つ以上のノードがあり、前の層のノードの値を入力として、次の層のノードの値を決めます。"
msgstr ""
"A neural network consists of an input layer, one or more intermediate (hidden) layers, and an output layer. "
"There are one or more nodes in each layer, and the values of nodes in the next layer is determined by the values of nodes in the previous layer as inputs."

#: ../../theory.rst:78
msgid "層 :math:`i` のノード :math:`j` の値を :math:`x_j^i` とすると、"
msgstr "Let :math:`x_j^i` be the value of the node :math:`j` in the layer :math:`i`\\ , then"

#: ../../theory.rst:80
msgid "x_j^{i+1} = f\\left(\\sum_k w_{jk}^i \\cdot x_k^i + b_k^i\\right)"
msgstr ""

#: ../../theory.rst:84
msgid ""
"となります。重み :math:`w` 、バイアス :math:`b` がニューラルネットワークのパラメータで、求める出力を得るために\\ "
":math:`w`\\ 、\\ :math:`b`\\ を最適化するのが「ニューラルネットワークの学習」ということになります。"
msgstr ""
"Weights :math:`w` and biases :math:`b` are parameters of a neural network, "
"and \"Training of a neural network\" is the optimization of :math:`w` and :math:`b` in order to acquire desired outputs."

#: ../../theory.rst:86
msgid "重みとバイアスに加え、非線形な振る舞いを表現するために使われるのが活性化関数 :math:`f` です。本製品では活性化関数として"
msgstr ""
"Activation functions :math:`f` are used to express nonlinear behaviors in addition to weights and biases. "
"This product allows users to select the activation functions itemized below."

#: ../../theory.rst:88
msgid "シグモイド関数 :math:`f(x)=1/(1+e^{-x})`"
msgstr "sigmoid function :math:`f(x)=1/(1+e^{-x})`"

#: ../../theory.rst:89
msgid "tanh関数 :math:`f(x)=\\tanh(x)`"
msgstr "tanh function :math:`f(x)=\\tanh(x)`"

#: ../../theory.rst:90
msgid "twisted tanh関数 :math:`f(x)=\\tanh(x)+\\alpha x, \\; \\alpha=0.16`"
msgstr "twisted tanh function :math:`f(x)=\\tanh(x)+\\alpha x, \\; \\alpha=0.16`"

#: ../../theory.rst:91
msgid ""
"eLU関数 :math:`f(x)=x \\; \\text{for} \\; x \\geqq 0; \\; e^x - 1 \\; "
"\\text{for} \\; x < 0`"
msgstr ""
"eLU function :math:`f(x)=x \\; \\text{for} \\; x \\geqq 0; \\; e^x - 1 \\; \\text{for} \\; x < 0`"

#: ../../theory.rst:92
msgid ""
"GELU関数 "
":math:`f(x)=(x/2)\\left(1+\\mathrm{erf}\\left(x/\\sqrt2\\right)\\right), "
"\\; \\mathrm{erf}(x)=\\left(2/\\sqrt\\pi\\right)\\int_{0}^{x} e^{-t^2} "
"\\mathrm{d}t`"
msgstr ""
"GELU function "
":math:`f(x)=(x/2)\\left(1+\\mathrm{erf}\\left(x/\\sqrt2\\right)\\right), "
"\\; \\mathrm{erf}(x)=\\left(2/\\sqrt\\pi\\right)\\int_{0}^{x} e^{-t^2} "
"\\mathrm{d}t`"

#: ../../theory.rst:94
msgid "または活性化関数なし :math:`f(x)=x` が選べます。"
msgstr "Or :math:`f(x)=x` without activation function can also be selected."

#: ../../theory.rst:99
msgid "SANNP"
msgstr ""

#: ../../theory.rst:101
msgid ""
"構造中のある原子 :math:`i` に対して、近傍の構造から対称関数を計算し、ニューラルネットワークに入力すると、出力としてその原子のエネルギー"
" :math:`E_i^\\text{NN}` が得られます。"
msgstr ""
"For a given atom :math:`i`\\ , the atomic energy :math:`E_i^\text{NN}` can be obtained as an output by calculating the symmetry functions from the neighboring structures and input them into the neural network."

#: ../../theory.rst:103
msgid ""
"一方、教師データとしては密度汎関数理論(DFT)に基づく第一原理計算が使われますが、その結果は「各原子のエネルギー」という形にはなっていません。系の全エネルギー"
" :math:`E_\\text{tot}^\\text{DFT}` を使い、 :math:`|E_\\text{tot}^\\text{DFT}"
" - \\sum_i E_{i}^\\text{NN}|` "
"を残差として最適化を行う方法がありますが、この場合1つの原子構造に対するDFT計算からエネルギーに関する情報は1つしか得られないことになります。"
msgstr ""
"On the other hand, first-principles calculations based on the Density Functional Theory (DFT) are used as training data, however, the result are not in the form of \"each atomic energy\". "
"Although there is the optimization method with :math:`|E_{tot}^\\text{DFT}-\\sum_i E_{i}^\\text{NN}|` as the residual by using the total energy in the system :math:`E_{tot}^\\text{DFT}`\\ , "
"only one information related to energy can be obtained from the DFT calculation for an atomic structure."

#: ../../theory.rst:105
msgid ""
"本製品で採用しているSANNP(Single Atom Neural Network Potential)\\ [3]_\\ "
"では、DFT計算の結果を各原子のエネルギー :math:`E_i^\\text{DFT}` に分割する手法\\ [5]_\\ "
"を使うことで、各原子のエネルギーを残差 :math:`|E_i^\\text{DFT} - E_i^\\text{NN}|` "
"を使って直接最適化しています。これにより、1つの原子構造に対するDFT計算から得られるエネルギーに関する情報が原子数倍になり、少ないDFT計算の結果からでも効率よく学習を行うことができます。"
msgstr ""
"With SANNP (Single Atom Neural Network Potential)\\ [3]_ adopted in this product, each atomic energy is directly optimized from the residual :math:`|E_i^\\text{DFT} - E_{i}^\\text{NN}|` by using the method of dividing the result of a DFT calculation into each atomic energy :math:`E_i^\\text{DFT}`\\ [5]_\\ . "
"This allows the information about the energy obtained from a DFT calculation for a single atomic structure to be the number of atoms times larger, making it possible to perform efficient learnings even from a small number of the results of DFT calculations."

#: ../../theory.rst:107
msgid "また、エネルギーと同様に、ニューラルネットワークを使って「各原子の電荷」を得ることができます。クーロン相互作用は長距離でも働くため、クーロン相互作用も含めてニューラルネットワーク力場で計算しようとすると大きなカットオフ半径が必要になります。本製品ではニューラルネットワーク力場と、ニューラルネットワークで計算した電荷を使ったクーロン相互作用を組み合わせて使うことができますので、短距離の相互作用を扱うニューラルネットワーク力場の部分についてはカットオフ半径を小さくして計算することが可能です。"
msgstr ""
"Moreover, as with the energy, \"each atomic charge\" can be obtained from a neural network. "
"Since Coulomb interactions work over long distances, a large cutoff radius is required to calculate the neural network force field, including Coulomb interactions. "
"Since this product can use a combination of neural network force field and Coulomb interaction using charges calculated by a neural network, it is possible to reduce the cutoff radius for the portion of the neural network force field that deals with short-range interactions."

#: ../../theory.rst:109
msgid "系の全エネルギーは、ニューラルネットワークで計算した各原子のエネルギー :math:`E_i^\\text{NN}` を使って、電荷を使わない場合"
msgstr "The total energy of the system is expressed with each atomic energy calculated from a neural network, :math:`E_i^\\text{NN}` as"

#: ../../theory.rst:111
msgid "E_\\text{tot}^\\text{NN} = \\sum_i E_i^\\text{NN}"
msgstr ""

#: ../../theory.rst:115
msgid ""
"と表現されます。また、ニューラルネットワークで計算した電荷 :math:`Q_i^\\text{NN}` "
"を使う場合は、系の全電荷が0になるようシフトした上で、"
msgstr ""
"without using charges. "
"While with using the charges calculated from a neural network, :math:`Q_i^\\text{NN}`\\ , it is calculated as"

#: ../../theory.rst:117
msgid ""
"E_\\text{tot}^\\text{NN} &= E_\\text{short} + E_\\text{elec} \\\\\n"
"&= \\sum_i E_i^\\text{NN} + \\sum_i \\sum_{j>i}^{R_{ij} < R_\\text{elec}}"
" \\frac{Q_i^\\text{NN} Q_j^\\text{NN}}{4\\pi\\epsilon_0R_{ij}} \\cdot "
"f_\\text{screen}(R_{ij}), \\\\\n"
"f_\\text{screen}&(R_{ij}) =\n"
"\\begin{cases}\n"
"\\frac{1}{2}\\left[1-\\cos\\left(\\frac{\\pi \\cdot "
"R_{ij}}{R_\\text{short}}\\right)\\right] \\; & \\text{for} \\; R_{ij} "
"\\leq R_\\text{short} \\\\\n"
"1 & \\text{for} \\; R_{ij} > R_\\text{short}\n"
"\\end{cases}"
msgstr ""

#: ../../theory.rst:127
msgid "として計算します。"
msgstr "after shifting the total charge of the system to zero."

#: ../../theory.rst:132
msgid "HDNNPにおける原子エネルギー推定法"
msgstr "Atomic Energy Estimation Method in HDNNP"

#: ../../theory.rst:134
msgid "NNPを最適化する際に、原子エネルギーの平均値および分散はニューラルネットワークの最終層の初期推定を行うのに重要な情報となります。第一原理計算によって得られる情報は、使用する計算手法(DFT+UやHybrid汎関数など)によっては十分な精度が得られず、ニューラルネットワークの学習の収束性が悪化してしまいます。また、VASP等の別のソフトウェアの結果を変換して教師データを得た場合にも、原子エネルギーのデータがないため、収束性が低下してしまいます。"
msgstr ""
"When optimizing NNPs, the mean and the variance of atomic energies are important information to perform an initial estimation of the final layer of a neural network. "
"The information obtained from first-principles calculations is not sufficiently accurate depending on the used calculation method (DFT+U, Hybrid Functional, etc.), which worsens the convergence of the neural network training. "
"Also, in the case that training data is obtained by converting the results from another software such as VASP, the convergence becomes worse due to the lack of atomic energy data."

#: ../../theory.rst:136
msgid "そこで、HDNNP（教師データとして系の全エネルギーを使う従来のNNP）使用時には、全エネルギーと化学量論係数から成る連立方程式を解いて原子エネルギーの平均値と分散を計算します。具体的には、下式を解きます。"
msgstr ""
"Therefore, when using HDNNP (conventional NNP that uses the total energy of the system as the training data), the mean and the variance of the atomic energies are calculated by solving a simultaneous equation consisting of the total energy and the stoichiometric coefficient. "
"Specifically, the equations below are solved."

#: ../../theory.rst:138
msgid "{}^\\mathrm{t}\\mathbf{XXe} = {}^\\mathrm{t}\\mathbf{XE}"
msgstr ""

#: ../../theory.rst:142
msgid ""
":math:`\\mathbf{e}`\\ は各元素種の原子エネルギーの平均値（ベクトル）、\\ :math:`\\mathbf{E}`\\ "
"は各教師データにおける全エネルギー（ベクトル）、\\ :math:`\\mathbf{X}`\\ "
"は各教師データにおける化学量論係数（教師データ数×元素種数の行列）です。\\ "
":math:`{}^\\mathrm{t}\\mathbf{XX}`\\ の一般化逆行列を左から両辺に作用させると\\ "
":math:`\\mathbf{e}`\\ "
"が求まります。分散についても同様の式を解きます。線形従属で無い限り、SANNPの原子エネルギーと概ね同じくらいのオーダーの値が得られる傾向にあります。"
msgstr ""
":math:`\\mathbf{e}` is the average atomic energy of each elemental species (vector), :math:`\\mathbf{E}` is the total energy in each training data (vector), and :math:`\\mathbf{X}` is the stoichiometric coefficient in each training data (matrix of the number of training data :math:`\\times` the number of elemental species). "
":math:`\\mathbf{e}` is obtained by acting generalized inverse of :math:`{}^\\mathrm{t}\\mathbf{XX}` on both sides from the left. "
"The similar equation is solved for the variance. "
"As long as it is not linearly dependent, it tends to be roughly the same order of magnitude as the atomic energy obtained by SANNP."

#: ../../theory.rst:147
msgid "メトロポリス法による構造生成"
msgstr "Structure Generation by Metropolis Method"

#: ../../theory.rst:149
msgid "元となる構造からランダムに原子を変位させて教師データ用の構造を生成する場合、その結果として生じる対称関数の分布、あるいはエネルギーの分布については考慮されません。そのため、構造自体は満遍ないように見えても、ある対象関数の範囲での学習が不十分になってしまう、ということがあります。"
msgstr ""
"When a structure for training data is generated by randomly displacing atoms from the original structure, the resultant distribution of symmetry functions or energies is not considered. "
"Therefore, it sometimes happens that the learning is insufficient in the certain range of symmetry functions although the structure itself appears to be uniformly distributed."

#: ../../theory.rst:151
msgid "本製品では、メトロポリス法による構造生成を行う機能があります。メトロポリス法では、確率的な遷移過程を使って、ボルツマン分布に従うエネルギー分布を持つように構造を生成することができます。"
msgstr ""
"This product provides the function to generate structures adopting the Metropolis method. "
"The Metropolis method uses stochastic transition processes to generate structures such that their energy distributions follow a Boltzmann distribution."

#: ../../theory.rst:153
msgid ""
"最初に元となる構造を考え、その構造を変化（原子の変位、または入れ替え）させて遷移先の候補となる新しい構造を作ります。学習済みのNNPを使って、それぞれの構造のエネルギー\\"
" :math:`E_\\mathrm{old}`\\ 、\\ :math:`E_\\mathrm{new}`\\ を計算します。エネルギーの差 "
":math:`\\Delta E=E_\\mathrm{new}-E_\\mathrm{old}` から、遷移確率\\ :math:`w`\\ "
"を次のように決めます："
msgstr ""
"Considering the original structure at first, then the structure is changed (displacement or replacement of atoms) to create a new structure that is a candidate for the transition destination. "
"The energy of each structure :math:`E_\\mathrm{old}` and :math:`E_\\mathrm{new}` are calculated by using the trained NNPs. "
"From the energy difference :math:`\\Delta E=E_\\mathrm{new}-E_\\mathrm{old}`\\ , the transition probability :math:`w` is determined as follows:"

#: ../../theory.rst:155
msgid ""
"w =\n"
"\\begin{cases}\n"
"1 &\\text{for} \\; \\Delta E\\leq 0, \\\\\n"
"\\exp(-\\Delta E/k_\\mathrm{B}T) &\\text{for} \\; \\Delta E>0.\n"
"\\end{cases} \\\\"
msgstr ""

#: ../../theory.rst:163
msgid ""
"この確率\\ :math:`w`\\ "
"によって、新しい構造に遷移する（採択）か、遷移しない（棄却）かを決めます。遷移したら、遷移先の構造を元として、また次の候補となる構造を作ります。"
msgstr ""
"This probability :math:`w` determines whether the new structure will be transitioned (adopted) or not (rejected). "
"Once a transition happens, another candidate structure is created based on the structure of the transitioned structure."

#: ../../theory.rst:165
msgid ""
"この手順を繰り返していくと、温度\\ :math:`T`\\ "
"でのボルツマン分布に従うようなエネルギー分布を持つ構造を生成することができます。この構造を元に教師データを作り、ニューラルネットワークを再度学習させる（強化学習）ことで、より広いエネルギーの構造をカバーするNNPを作ることができます。"
msgstr ""
"Repeating this procedure can produce a structure such that its energy distribution follows the Boltzmann distribution at temperature :math:`T`\\ . "
"Generating training data based on this structure and retraining the neural network (reinforcement learning) can create an NNP that covers a wider range of energy structures."

#: ../../theory.rst:170
msgid "|Delta|\\ -NNP"
msgstr ""

#: ../../theory.rst:172
msgid "全エネルギーをNNPで表現するのではなく、第0近似として古典力場による2体間エネルギーを使い、そこからの差分をNNPを使って表現する、という方法です。"
msgstr "|Delta|\\ -NNP is the method to express in NNP the difference from the energy between two bodies due to the classical force field as the zeroth approximation instead of the total energy."

#: ../../theory.rst:174
msgid ""
"E_\\mathrm{tot} &= E_\\mathrm{classical}+E_\\mathrm{NNP},\n"
"\n"
"E_\\mathrm{classical} &= "
"\\frac{1}{2}\\sum_{i,j}\\frac{A}{r_{ij}^{12}}+\\frac{B}{r_{ij}^{10}}+\\frac{C}{r_{ij}^8}+\\frac{D}{r_{ij}^6}"
msgstr ""

#: ../../theory.rst:180
msgid ""
"古典力場のパラメータについては、教師データを使ってあらかじめ最適化しておきます。ニューラルネットワークを学習する際の教師データには、DFT計算と古典力場のエネルギーの差分"
" :math:`E_\\mathrm{DFT}-E_\\mathrm{classical}` を使います。"
msgstr ""
"The parameters of the classical force field are optimized in advance using training data. "
"The energy difference between the DFT calculation and the classical force field :math:`E_\\mathrm{DFT}-E_\\mathrm{classical}` is used as training data when training a neural network."

#: ../../theory.rst:182
msgid ""
"|Delta|\\ "
"-NNPは通常のNNPに比べてロバストで、少ない教師データで作成した力場でも、大きな破綻が起きにくいのが特長です。外挿も機能し、例えば300 "
"Kの教師データで作成した力場でも、1000 Kでそれなりに上手く動きます。"
msgstr ""
"|Delta|\\ -NNP is more robust than regular NNP and less prone to major breakdowns in force fields created with less training data. "
"Extrapolation also works, for example, a force field created with training data at 300 K also works reasonably well at 1000 K."

msgid "|Delta|\\ -NNPとHDNNPの収束性の比較"
msgstr "Comparison of convergence between |Delta|\\ -NNP and HDNNP"

#: ../../theory.rst:188
msgid ""
"図： |Delta|\\ -NNPとHDNNPを同じ教師データ（Li\\ `10`:sub:\\ GeP\\ `2`:sub:\\ S\\ "
"`12`:sub:\\ 、6914構造）を使って学習し、RMSEを比較しました。\\ |Delta|\\ "
"-NNPはHDNNPよりも収束性が良いことが確認できます。"
msgstr ""
"Figure: |Delta|\\ -NNP and HDNNP were trained using the same training data (Li\\ `10`:sub:\\ GeP\\ `2`:sub:\\ S\\ "
"`12`:sub:\\ , 6914 structures) and the RMSE was compared. |Delta|\\ -NNP showed better convergence than HDNNP."

#: ../../theory.rst:193
msgid "自己学習ハイブリッドモンテカルロ法"
msgstr "Self-learning Hybrid Monte Carlo Method "

#: ../../theory.rst:195
msgid ""
"自己学習ハイブリッドモンテカルロ(Self-learning hybrid Monte Carlo, SLHMC)法\\ [6]_\\ "
"は、教師データの生成とニューラルネットワーク力場の学習を同時に進めることができる手法です。以下のようなアルゴリズムに従って実行します。"
msgstr ""
"The Self-learning hybrid Monte Carlo (SLHMC) method\\ [6]_ is the method that can proceed simultaneous generation of training data and learning of neural network force fields. "
"It is executed according to the following algorithm."

#: ../../theory.rst:197
msgid "初期構造から第一原理分子動力学計算をある程度進め、それを教師データとして初期ニューラルネットワーク力場を作る。"
msgstr ""
"Advance some ab initio molecular dynamics calculations from the initial structure and use them as training data to create an initial neural network force field."

#: ../../theory.rst:198
msgid ""
"ニューラルネットワーク力場による分子動力学計算(NVE)で構造を時間発展させる( :math:`\\{\\bm{p}, \\bm{r}\\} "
"\\rightarrow \\{\\bm{p}', \\bm{r}'\\}` )。"
msgstr ""
"The structure is evolved in time by first-principles molecular dynamics calculations (NVE) using the neural network force field ( :math:`\\{\\bm{p}, \\bm{r}\\} \\rightarrow \\{\\bm{p}', \\bm{r}'\\}` )."

#: ../../theory.rst:199
msgid ""
"発展後の構造 :math:`\\{\\bm{p}', \\bm{r}'\\}` について第一原理計算でエネルギー "
":math:`H_\\mathrm{DFT}(\\{\\bm{p}', \\bm{r}'\\})` を計算する。"
msgstr ""
"Calculate the energy :math:`H_\\mathrm{DFT}(\\{\\bm{p}', \\bm{r}'\\})` for the structure :math:`\\{\\bm{p}', \\bm{r}'\\}` after time development by first-principles calculation."

#: ../../theory.rst:200
msgid "以下の遷移確率 :math:`P_\\mathrm{acc}` を使って、メトロポリス法により構造を採択するかどうか決める。"
msgstr "Use the following transition probabilities :math:`P_\\mathrm{acc}` to decide whether to adopt the structure by the Metropolis method."

#: ../../theory.rst:202
msgid ""
"P_\\mathrm{acc} = \\min(1, e^{-(H_\\mathrm{DFT}(\\{\\bm{p}', "
"\\bm{r}'\\})-H_\\mathrm{DFT}(\\{\\bm{p}, \\bm{r}\\}))/k_\\mathrm{B}T})"
msgstr ""

#: ../../theory.rst:206
msgid ""
"棄却する場合は、構造を :math:`\\{\\bm{p}, \\bm{r}\\}` に戻し、運動量 :math:`\\bm{p}` を温度 "
":math:`T` におけるボルツマン分布で定義しなおす。"
msgstr ""
"If it is rejected, return the structure to :math:`\\{\\bm{p}, \\bm{r}\\}` and redefine the momentum :math:`\\bm{p}` with the Boltzmann distribution at temperature :math:`T`\\ ."

#: ../../theory.rst:207
msgid "1.~3.をある程度繰り返したら、生成された構造（採択された構造のみ、または棄却されたものも含めた全ての構造）を教師データとしてニューラルネットワーク力場の再学習を行う。"
msgstr ""
"After repeating 1.~3. to some extent, retrain the neural network force field using the generated structures (only the adopted structures or all the structures including the rejected ones) as the training data."

#: ../../theory.rst:208
msgid "1.~3.と4.を繰り返す。"
msgstr "Repeat 1.~3. and 4."

#: ../../theory.rst:210
msgid "この方法では、教師データの準備すら行う必要がなく、1つの初期構造を用意するだけで後は自動的にニューラルネットワーク力場を作ることができます。力場作成の手間が大幅に削減でき、また教師データの質など手順に依存する要素が排除されるため、常に同水準の力場を作ることができます。教師データの数も低減できる傾向にあるため、計算に要する時間も大幅に削減できます。"
msgstr ""
"In this method, there is no need to even prepare the training data but only one initial structure, and then a neural network force field can be automatically created. "
"A force field can always be created to the same standard because the time and effort required to create a force field can be greatly reduced, and that factors depending on the procedure such as the quality of training data are eliminated. "
"Moreover, the time required for calculations can also be significantly reduced since it tends to be possible to reduce the number of training data."

#: ../../theory.rst:212
msgid "モンテカルロ法の遷移確率(3.)は第一原理計算により決めているため、教師データのアンサンブルが厳密に第一原理分子動力学計算のアンサンブルと一致するということもこの方法の特長です。"
msgstr ""
"Another feature of this method is that the ensemble of training data strictly matches the ensemble of the first-principles molecular dynamics calculations, since the transition probabilities (3.) of the Monte Carlo method are determined by first-principles calculations."

#: ../../theory.rst:214
msgid "また、本製品では構造の時間発展(1.)の前に、NPHによるセルの変形を行うこともできます。これによりセル形状の異なる教師データも含まれるようになり、格子定数（密度）が未知の物質に対しても本手法を使って力場を作ることができるようになります。この場合、厳密なモンテカルロ法ではなくなりますが、力場を作るという目的の上では問題なく使えます。"
msgstr ""
"This product can also deform the cell by NPH before the time development of the structure (1.). "
"This will also include training data with different cell geometries, allowing this method to create force fields for materials with unknown lattice constants (densities). "
"In this case, it is no longer a strict Monte Carlo method, but it can be used without problems for the purpose of creating a force field."

#: ../../theory.rst:216
msgid ""
"Behler, \"Constructing high‐dimensional neural network potentials: A "
"tutorial review\", Int. J. Quantum Chem. **115**, 1032-1050 (2015). DOI: "
"`10.1002/qua.24890 <https://doi.org/10.1002/qua.24890>`_"
msgstr ""

#: ../../theory.rst:217
msgid ""
"Artrith *et al.*, \"Efficient and accurate machine-learning interpolation"
" of atomic energies in compositions with many species\", Phys. Rev. B "
"**96**, 014112 (2017). DOI: `10.1103/PhysRevB.96.014112 "
"<https://doi.org/10.1103/PhysRevB.96.014112>`_"
msgstr ""

#: ../../theory.rst:218
msgid ""
"Huang *et al.*, \"Density functional theory based neural network force "
"fields from energy decompositions\", Phys. Rev. B **99**, 064103 (2019). "
"DOI: `10.1103/PhysRevB.99.064103 "
"<https://doi.org/10.1103/PhysRevB.99.064103>`_"
msgstr ""

#: ../../theory.rst:219
msgid ""
"Gastegger *et al.*, \"wACSF—Weighted atom-centered symmetry functions as "
"descriptors in machine learning potentials\", J. Chem. Phys. **148**, "
"241709 (2018). DOI: `10.1063/1.5019667 "
"<https://doi.org/10.1063/1.5019667>`_"
msgstr ""

#: ../../theory.rst:220
msgid ""
"Kang and L.-W. Wang, \"First-principles green-Kubo method for thermal "
"conductivity calculations\", Phys. Rev. B **96**, 020302 (2017). DOI: "
"`10.1103/PhysRevB.96.020302 "
"<https://doi.org/10.1103/PhysRevB.96.020302>`_"
msgstr ""

#: ../../theory.rst:221
msgid ""
"Nagai *et al.*, \"Self-learning hybrid Monte Carlo: A first-principles "
"approach\", Phys. Rev. B **102**, 041124 (2020). DOI: "
"`10.1103/PhysRevB.102.041124 "
"<https://doi.org/10.1103/PhysRevB.102.041124>`_"
msgstr ""

